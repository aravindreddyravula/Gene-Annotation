{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\">Neural Network Implementation Basics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importing packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading Files </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = pd.read_fwf('../positive_sample.txt', header = None, nrows = 500)\n",
    "negative_data = pd.read_fwf('../negative_sample.txt', header = None, nrows = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generating vocabulary </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()\n",
    "window_size = 3\n",
    "embeddings_size = 5\n",
    "def generate_vocabmap_helper(set, k): \n",
    "    n = len(set)  \n",
    "    generate_k_sized(set, \"\", n, k) \n",
    "def generate_k_sized(set, prefix, n, k): \n",
    "    if (k == 0) : \n",
    "        vocab_set.add(prefix)\n",
    "        return\n",
    "    for i in range(n): \n",
    "        newPrefix = prefix + set[i] \n",
    "        generate_k_sized(set, newPrefix, n, k - 1) \n",
    "def generate_vocabmap(n):\n",
    "    alphabet = ['0','1','2','3','4']\n",
    "    generate_vocabmap_helper(alphabet, n)\n",
    "    \n",
    "    vocab_set_1 = sorted(vocab_set)\n",
    "    vocab_map = {}\n",
    "    \n",
    "    for i in range(len(vocab_set_1)):\n",
    "        vocab_map[vocab_set_1[i]] = i\n",
    "    return vocab_map\n",
    "\n",
    "#Call for generating vocabulary\n",
    "vocabulary = generate_vocabmap(window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generating Embeddings </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding matrix for input and ouput\n",
    "embeds = nn.Embedding(len(vocabulary), embeddings_size)\n",
    "embeddings = {}\n",
    "for word in vocabulary:\n",
    "    embeddings[word] = embeds(torch.tensor(vocabulary[word], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_window_length_bases(data_sample):\n",
    "    list_of_tensors = []\n",
    "    for data in data_sample.itertuples():\n",
    "        for i in range(len(data[1]) - window_size + 1):\n",
    "            if i == 0:\n",
    "                first_tensor = embeddings[data[1][i:i+window_size]]\n",
    "            else:\n",
    "                first_tensor = torch.cat((first_tensor, embeddings[data[1][i:i+window_size]]), 0)\n",
    "        list_of_tensors.append(first_tensor)\n",
    "    train = torch.stack(list_of_tensors)\n",
    "    return train\n",
    "positives = generate_window_length_bases(positive_data)\n",
    "negatives = generate_window_length_bases(negative_data)\n",
    "data_ = torch.cat([positives, negatives], dim=0)\n",
    "\n",
    "negative_labels = torch.zeros(negatives.shape[0], 1)\n",
    "positive_labels = torch.ones(positives.shape[0], 1)\n",
    "labels_ = torch.cat([positive_labels, negative_labels], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_.shape\n",
    "data_.shape\n",
    "print(len(positive_data[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> FC Layer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(embeddings_size * (len(positive_data[0][0]) - window_size + 1), 505)\n",
    "        self.relu1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(505, 100)\n",
    "        self.prelu = nn.ReLU()\n",
    "        self.out = nn.Linear(100, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.fc1(x)\n",
    "        h1 = self.relu1(a1)\n",
    "        #dout = self.dout(h1)\n",
    "        a2 = self.fc2(h1)\n",
    "        h2 = self.prelu(a2)\n",
    "        a3 = self.out(h2)\n",
    "        y = self.out_act(a3)\n",
    "        return y\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, criterion, batch_size=50):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i in range(0, data_.size(0), batch_size):\n",
    "        data_batch = data_[i:i + batch_size, :]\n",
    "        labels_batch = labels_[i:i + batch_size, :]\n",
    "        data_batch = autograd.Variable(data_batch)\n",
    "        labels_batch = autograd.Variable(labels_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # (1) Forward\n",
    "        labels_hat = net(data_batch)\n",
    "        # (2) Compute diff\n",
    "        loss = criterion(labels_hat, labels_batch)\n",
    "        # (3) Compute gradients\n",
    "        loss.backward()\n",
    "        # (4) update weights\n",
    "        opt.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "    loss = sum(losses)/len(losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epochs = 1000\n",
    "for e in range(epochs):\n",
    "    losses.append(train_epoch(net, opt, criterion))\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
